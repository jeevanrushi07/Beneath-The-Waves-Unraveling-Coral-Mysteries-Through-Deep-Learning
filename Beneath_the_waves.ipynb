{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbApZBmqo08z",
    "outputId": "c946d4e5-4a1a-436d-fa07-96921acdd8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N7ocePwTpVJv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image  import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import softmax\n",
    "from keras.api._v2.keras import activations\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_1XG8yk2C6-",
    "outputId": "49854426-c87b-4cc0-a223-54c0af12391b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 738\n",
      "Total testing images: 185\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files containing image labels\n",
    "train_labels = pd.read_csv('/content/train.csv')\n",
    "test_labels = pd.read_csv('/content/test.csv')\n",
    "\n",
    "# Display the total number of images and classes\n",
    "total_train_images = len(train_labels)\n",
    "total_test_images = len(test_labels)\n",
    "num_classes = len(train_labels['label'].unique())\n",
    "\n",
    "print(f'Total training images: {total_train_images}')\n",
    "print(f'Total testing images: {total_test_images}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xjmge1pdYCpX"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "rescale=1./255,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,\n",
    "validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KUG5AW83YMi2"
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqxIfLmdYNNf",
    "outputId": "760a1607-e78a-4976-bdf4-abd7df97e6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 601 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#x_train = train_datagen.flow_from_directory()\n",
    "train_set = train_datagen.flow_from_directory('/content/drive/MyDrive/train',\n",
    "                                              target_size=(299,299),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode= 'categorical',\n",
    "                                              subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgdfJPdXfQhO",
    "outputId": "89a5a39f-c20d-4eaa-e950-b68dbf41e73d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleached': 0, 'healthy': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PUDEaxTfWS0",
    "outputId": "b1dfaf69-ce10-4978-c20f-bf7511a13416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/test',\n",
    "                                            target_size=(299,299),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='categorical',\n",
    "                                            subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyTgC3v90-it"
   },
   "source": [
    "#**VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Utj6mZZqfqBI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D,Flatten,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "srg2OANXfvJ0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RL8vptLpfyzh",
    "outputId": "530db559-b283-49d5-9327-6db5bcb73119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG16(weights='imagenet',include_top = False,input_shape = (299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkC5M6sof4Tm",
    "outputId": "7bbeab69-2259-43da-a61b-522d415775cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.engine.input_layer.InputLayer object at 0x7d1e88cc3280>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfd096e90>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfd097610>\n",
      "<keras.src.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7d1dfc7c4af0>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfd097820>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfc7c51b0>\n",
      "<keras.src.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7d1dfc7c68c0>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfc7c7130>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfc7c66e0>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfc7c7dc0>\n",
      "<keras.src.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7d1df6740ca0>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1dfc7c4e20>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1df6741c90>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1df67427a0>\n",
      "<keras.src.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7d1df6743850>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1df6743e20>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1df67435e0>\n",
      "<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7d1df67542b0>\n",
      "<keras.src.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7d1df67562f0>\n"
     ]
    }
   ],
   "source": [
    "for layer in vgg.layers:\n",
    "  print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0X91UjsqIER",
    "outputId": "94ffbc8c-9b06-4197-c534-09ee8d05a9a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vgg.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9XHk-yNlGZf",
    "outputId": "1524108a-ff4c-4f30-9f69-53a6e3a69a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 9, 9, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               10617088  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25332290 (96.64 MB)\n",
      "Trainable params: 10617602 (40.50 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers of the pre-trained model\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model using the VGG16 base\n",
    "vggmodel = models.Sequential()\n",
    "vggmodel.add(vgg)\n",
    "\n",
    "# Flatten the output and add dense layers for classification\n",
    "vggmodel.add(layers.Flatten())\n",
    "vggmodel.add(layers.Dense(256, activation='relu'))\n",
    "vggmodel.add(layers.Dropout(0.5))\n",
    "vggmodel.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "vggmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0kuGlz_nj0Z",
    "outputId": "208d5f8b-794a-4528-d42b-96e3d4fccb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 227s 12s/step - loss: 1.9309 - accuracy: 0.5852 - val_loss: 0.5904 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 16s 888ms/step - loss: 0.6249 - accuracy: 0.6678 - val_loss: 0.5926 - val_accuracy: 0.6875\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 17s 919ms/step - loss: 0.5591 - accuracy: 0.7170 - val_loss: 0.6045 - val_accuracy: 0.6562\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 17s 966ms/step - loss: 0.5431 - accuracy: 0.7311 - val_loss: 0.5734 - val_accuracy: 0.6875\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 18s 989ms/step - loss: 0.5087 - accuracy: 0.7452 - val_loss: 0.5552 - val_accuracy: 0.7188\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 16s 900ms/step - loss: 0.4507 - accuracy: 0.7909 - val_loss: 0.5041 - val_accuracy: 0.6875\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 18s 986ms/step - loss: 0.4955 - accuracy: 0.7504 - val_loss: 0.5255 - val_accuracy: 0.6875\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 16s 897ms/step - loss: 0.4430 - accuracy: 0.7786 - val_loss: 0.5944 - val_accuracy: 0.7188\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 17s 973ms/step - loss: 0.4110 - accuracy: 0.8172 - val_loss: 0.5671 - val_accuracy: 0.7812\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 18s 980ms/step - loss: 0.4107 - accuracy: 0.8172 - val_loss: 0.5559 - val_accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = vggmodel.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=test_set.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IxVYGga9gSGD"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "#opt = Adam(lr=0.001)\n",
    "\n",
    "vggmodel.compile(loss = 'categorical_crossentropy' , metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4AY5gO0tggvC"
   },
   "outputs": [],
   "source": [
    "labels = ['bleached','healthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UKcUNZwpednJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9whG7n8PgtAn",
    "outputId": "8a589e72-ac7a-4c0d-9b4d-cb13c282198b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 238ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.6030145e-15, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "img = '/content/drive/MyDrive/test/healthy/14307809941_27b4257449_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds = vggmodel.predict(np.array([x]))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "InzR5X6NGigf",
    "outputId": "626adc1f-d112-4b60-b34a-27fa35a2d320"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBu7iC2Bzi-y",
    "outputId": "7aaa32f3-a7a5-45a8-bc45-81493487ba68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.8775461e-17, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "img = '/content/drive/MyDrive/test/bleached/14633816529_af934e0aa8_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds1 = vggmodel.predict(np.array([x]))\n",
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EYixddtCzpSN",
    "outputId": "0296ceb7-ca81-4fcc-81b7-3a60376e362a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Fgqf__dr9Wtd"
   },
   "outputs": [],
   "source": [
    "vggmodel.save('waves_img.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PjFmpwktFeys"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "# # Define image dimensions\n",
    "# IMG_HEIGHT = 224\n",
    "# IMG_WIDTH = 224\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# # Data generators with augmentation\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest',\n",
    "#     validation_split=0.2\n",
    "# )\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     '/content/drive/MyDrive/train',\n",
    "#     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     class_mode='binary',  # or 'categorical' if you have more than 2 classes\n",
    "#     subset='training'\n",
    "# )\n",
    "\n",
    "# validation_generator = train_datagen.flow_from_directory(\n",
    "#     '/content/drive/MyDrive/test',\n",
    "#     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     class_mode='binary',  # or 'categorical' if you have more than 2 classes\n",
    "#     subset='validation'\n",
    "# )\n",
    "\n",
    "# # Define the model (example using VGG16)\n",
    "# vgg_model = tf.keras.applications.VGG16(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# # Unfreeze the top layers of the VGG model for fine-tuning\n",
    "# for layer in vgg_model.layers[-4:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     vgg_model,\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(512, activation='relu'),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(256, activation='relu'),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')  # or more units for multiple classes with 'softmax'\n",
    "# ])\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# def lr_schedule(epoch, lr):\n",
    "#     if epoch > 10:\n",
    "#         lr = lr * tf.math.exp(-0.1)\n",
    "#     return lr\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "# lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "#     epochs=20,  # Reduced number of epochs\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "#     callbacks=[early_stopping, checkpoint, lr_scheduler]\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc-8i1Awv5yf"
   },
   "source": [
    "#**Resnet**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnyw5vi7v-4a",
    "outputId": "e1cad11d-f45c-4b03-c602-ee7a178c3f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 4s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 10, 10, 2048)      23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 204800)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               52429056  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76017282 (289.98 MB)\n",
      "Trainable params: 52429570 (200.00 MB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model using the ResNet50 base\n",
    "resnet_model = models.Sequential()\n",
    "resnet_model.add(resnet)\n",
    "\n",
    "# Flatten the output and add dense layers for classification\n",
    "resnet_model.add(layers.Flatten())\n",
    "resnet_model.add(layers.Dense(256, activation='relu'))\n",
    "resnet_model.add(layers.Dropout(0.5))\n",
    "resnet_model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "resnet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSirmjLEwedV",
    "outputId": "844a04f0-6d8a-49fd-a120-44b15e77ca90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 27s 1s/step - loss: 10.8581 - accuracy: 0.5026 - val_loss: 0.8497 - val_accuracy: 0.5312\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 17s 972ms/step - loss: 1.0101 - accuracy: 0.5272 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 16s 875ms/step - loss: 0.7763 - accuracy: 0.5220 - val_loss: 0.6928 - val_accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 16s 888ms/step - loss: 0.6931 - accuracy: 0.5132 - val_loss: 0.6930 - val_accuracy: 0.5625\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 17s 960ms/step - loss: 0.6931 - accuracy: 0.5202 - val_loss: 0.6929 - val_accuracy: 0.5625\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 16s 911ms/step - loss: 0.6931 - accuracy: 0.5185 - val_loss: 0.6929 - val_accuracy: 0.5312\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 17s 971ms/step - loss: 0.6930 - accuracy: 0.5202 - val_loss: 0.6926 - val_accuracy: 0.5625\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 17s 957ms/step - loss: 0.6930 - accuracy: 0.5202 - val_loss: 0.6925 - val_accuracy: 0.5625\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 17s 972ms/step - loss: 0.6929 - accuracy: 0.5202 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 16s 889ms/step - loss: 0.8487 - accuracy: 0.5220 - val_loss: 0.6921 - val_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32  # Adjust the batch size based on your requirements\n",
    "\n",
    "history= resnet_model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=test_set.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZixrjcCuyxHV"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "#opt = Adam(lr=0.001)\n",
    "\n",
    "resnet_model.compile(loss = 'categorical_crossentropy' , metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "oV5dqN0_y2eu"
   },
   "outputs": [],
   "source": [
    "labels = ['bleached','healthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rb1YMe7vy8Sx",
    "outputId": "6bf33bea-0271-4f2b-d0c3-398517071823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.19680645, 0.3545633 ], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/healthy/16426224025_f750fb7e32_b.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds = resnet_model.predict(np.array([x])).flatten()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ll9clXEUv-Ix",
    "outputId": "2c47ef8b-8fc3-4712-8d6b-ea80aafa697b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtO3ZYqpfB6x",
    "outputId": "f625d114-556f-4f2e-f16e-b0557458144d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 181ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.18538068, 0.19686073], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/bleached/14157549490_03cdd7426c_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds1= resnet_model.predict(np.array([x])).flatten()\n",
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5LfFm1sMfEor",
    "outputId": "49e2a371-f2c2-40f8-f98b-ee0c31855658"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoFjVyU-3gAG"
   },
   "source": [
    "#**Inception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwWx5nit3fL-",
    "outputId": "d2dc4402-20e2-4400-f346-c8ab8ff4bc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 3s 0us/step\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 2048)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22327842 (85.17 MB)\n",
      "Trainable params: 525058 (2.00 MB)\n",
      "Non-trainable params: 21802784 (83.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Load the pre-trained InceptionV3 model\n",
    "inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model using the InceptionV3 base\n",
    "inception_model = models.Sequential()\n",
    "inception_model.add(inception)\n",
    "\n",
    "# Add a Global Average Pooling layer\n",
    "inception_model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "# Add dense layers for classification\n",
    "inception_model.add(layers.Dense(256, activation='relu'))\n",
    "inception_model.add(layers.Dropout(0.5))\n",
    "inception_model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "inception_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "inception_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvA3GHnM3Tfc",
    "outputId": "e803f6d2-ea58-4b96-998b-45df6ac02b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 30s 1s/step - loss: 0.7325 - accuracy: 0.5852 - val_loss: 0.5530 - val_accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 15s 857ms/step - loss: 0.5871 - accuracy: 0.6766 - val_loss: 0.5069 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 15s 861ms/step - loss: 0.5599 - accuracy: 0.7276 - val_loss: 0.5826 - val_accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 16s 887ms/step - loss: 0.5296 - accuracy: 0.7592 - val_loss: 0.5676 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 16s 876ms/step - loss: 0.5220 - accuracy: 0.7346 - val_loss: 0.5709 - val_accuracy: 0.6562\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 17s 943ms/step - loss: 0.5226 - accuracy: 0.7135 - val_loss: 0.4757 - val_accuracy: 0.8438\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 16s 888ms/step - loss: 0.4835 - accuracy: 0.7645 - val_loss: 0.4888 - val_accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 16s 874ms/step - loss: 0.5064 - accuracy: 0.7469 - val_loss: 0.5057 - val_accuracy: 0.7188\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 16s 865ms/step - loss: 0.5018 - accuracy: 0.7487 - val_loss: 0.5024 - val_accuracy: 0.6875\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 16s 867ms/step - loss: 0.4940 - accuracy: 0.7417 - val_loss: 0.4842 - val_accuracy: 0.8438\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32  # Adjust the batch size based on your requirements\n",
    "\n",
    "history= inception_model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=test_set.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7-hsWJ3k-ms_"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "#opt = Adam(lr=0.001)\n",
    "\n",
    "inception_model.compile(loss = 'categorical_crossentropy' , metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "UjLz1j8S-7o5"
   },
   "outputs": [],
   "source": [
    "labels = ['bleached','healthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ie693HSL_Grz",
    "outputId": "f1acfb5a-c05c-45e4-c367-ec97d2d82487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08862984, 0.13824932], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/healthy/11181383133_411acd063f_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds = inception_model.predict(np.array([x])).flatten()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8tBhGpsY_Hd9",
    "outputId": "416f9f2e-86ed-4cd2-9721-92c6db2ec87f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1HNFIYy1wtA",
    "outputId": "3b0727e6-8b2e-45be-9bdb-6c5bf35562eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.999945e-01, 5.723633e-08], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/bleached/14157549490_03cdd7426c_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds1 = inception_model.predict(np.array([x])).flatten()\n",
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lUHZiIyn12fM",
    "outputId": "8759c1e8-7366-499b-8657-52214bd42acb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'bleached'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZEVioki_luJ"
   },
   "source": [
    "#**Densenet**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U99IAZmE_k0A",
    "outputId": "5c6c3375-0f04-4b31-f724-6eea57307f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29084464/29084464 [==============================] - 2s 0us/step\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet121 (Functional)    (None, 9, 9, 1024)        7037504   \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1024)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7300418 (27.85 MB)\n",
      "Trainable params: 262914 (1.00 MB)\n",
      "Non-trainable params: 7037504 (26.85 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Load the pre-trained DenseNet121 model\n",
    "densenet = DenseNet121(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in densenet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model using the DenseNet121 base\n",
    "densenet_model = models.Sequential()\n",
    "densenet_model.add(densenet)\n",
    "\n",
    "# Add a Global Average Pooling layer\n",
    "densenet_model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "# Add dense layers for classification\n",
    "densenet_model.add(layers.Dense(256, activation='relu'))\n",
    "densenet_model.add(layers.Dropout(0.5))\n",
    "densenet_model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "densenet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "densenet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYmN2s9V_316",
    "outputId": "f3310e42-585f-428f-b853-4dd0f67a7f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 39s 1s/step - loss: 0.6469 - accuracy: 0.6555 - val_loss: 0.6380 - val_accuracy: 0.6562\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 16s 869ms/step - loss: 0.5510 - accuracy: 0.7258 - val_loss: 0.5677 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 17s 899ms/step - loss: 0.4954 - accuracy: 0.7610 - val_loss: 0.5721 - val_accuracy: 0.6875\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 16s 877ms/step - loss: 0.4635 - accuracy: 0.8137 - val_loss: 0.4976 - val_accuracy: 0.7188\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 16s 879ms/step - loss: 0.4449 - accuracy: 0.7768 - val_loss: 0.5479 - val_accuracy: 0.8125\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 16s 864ms/step - loss: 0.4366 - accuracy: 0.7856 - val_loss: 0.5005 - val_accuracy: 0.7188\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 16s 877ms/step - loss: 0.4236 - accuracy: 0.7873 - val_loss: 0.5262 - val_accuracy: 0.7188\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 16s 871ms/step - loss: 0.3985 - accuracy: 0.8225 - val_loss: 0.4691 - val_accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 16s 879ms/step - loss: 0.3898 - accuracy: 0.8032 - val_loss: 0.4638 - val_accuracy: 0.7812\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 16s 861ms/step - loss: 0.3873 - accuracy: 0.8243 - val_loss: 0.5603 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32  # Adjust the batch size based on your requirements\n",
    "\n",
    "history= densenet_model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=test_set.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRl6w9ptAAp-",
    "outputId": "699eff85-dfff-4d59-90e7-d8c8e4fb9347"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d1d620e5f30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00666854, 0.8683572 ], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/healthy/11181383133_411acd063f_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds = densenet_model.predict(np.array([x])).flatten()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FmYsRbwB28pe",
    "outputId": "60939a38-ee94-4035-e0ab-d6b23f7e4e8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jYWxDwo3Dc1",
    "outputId": "0497ed62-171f-454d-8d7d-28c2db6e7192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.993168e-01, 7.462764e-05], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/bleached/14157549490_03cdd7426c_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds1 = densenet_model.predict(np.array([x])).flatten()\n",
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "I124wNlK3LQR",
    "outputId": "aadf3c58-219a-4f41-93c2-b35ca111daa7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'bleached'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLvAc0lOBo2g"
   },
   "source": [
    "#**Xpection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M65HFrmiBsFG",
    "outputId": "722d8c5c-b7d2-4005-d4b0-8cfcbfa48401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83683744/83683744 [==============================] - 4s 0us/step\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 10, 10, 2048)      20861480  \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21386538 (81.58 MB)\n",
      "Trainable params: 525058 (2.00 MB)\n",
      "Non-trainable params: 20861480 (79.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Load the pre-trained Xception model\n",
    "xception = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in xception.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model using the Xception base\n",
    "xception_model = models.Sequential()\n",
    "xception_model.add(xception)\n",
    "\n",
    "# Add a Global Average Pooling layer\n",
    "xception_model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "# Add dense layers for classification\n",
    "xception_model.add(layers.Dense(256, activation='relu'))\n",
    "xception_model.add(layers.Dropout(0.5))\n",
    "xception_model.add(layers.Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "xception_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "xception_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYxYK5HKBvgD",
    "outputId": "93182b8c-72b6-4305-946a-a35426949f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 29s 1s/step - loss: 0.6391 - accuracy: 0.6432 - val_loss: 0.6444 - val_accuracy: 0.5938\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 17s 946ms/step - loss: 0.5279 - accuracy: 0.7223 - val_loss: 0.6830 - val_accuracy: 0.6562\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 18s 966ms/step - loss: 0.5242 - accuracy: 0.7522 - val_loss: 0.7006 - val_accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 17s 967ms/step - loss: 0.4873 - accuracy: 0.7715 - val_loss: 0.6069 - val_accuracy: 0.6562\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 18s 1s/step - loss: 0.4703 - accuracy: 0.7891 - val_loss: 0.7219 - val_accuracy: 0.5938\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 17s 977ms/step - loss: 0.4619 - accuracy: 0.7768 - val_loss: 0.6721 - val_accuracy: 0.6562\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 17s 960ms/step - loss: 0.4266 - accuracy: 0.8120 - val_loss: 0.5425 - val_accuracy: 0.7188\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 18s 956ms/step - loss: 0.4251 - accuracy: 0.8084 - val_loss: 0.5700 - val_accuracy: 0.6250\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 18s 983ms/step - loss: 0.3955 - accuracy: 0.8190 - val_loss: 0.5975 - val_accuracy: 0.6562\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 16s 902ms/step - loss: 0.4082 - accuracy: 0.8225 - val_loss: 0.6815 - val_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32  # Adjust the batch size based on your requirements\n",
    "\n",
    "history= xception_model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=test_set.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5bG7uosCmic",
    "outputId": "f8f0a17e-807f-4e0d-a212-5156a629dfc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d1c99fa3f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.91707754, 0.46083626], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/bleached/14157549490_03cdd7426c_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds = xception_model.predict(np.array([x])).flatten()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GM0DFYVsEuk9",
    "outputId": "299fe690-f284-4498-8d89-26f3a4010a77"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'bleached'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6OmVbDe4fPm",
    "outputId": "0e3481b1-e290-4571-f920-68b27b50840d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.864559e-07, 9.999789e-01], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = '/content/drive/MyDrive/test/healthy/11181383133_411acd063f_o.jpg'\n",
    "img = load_img(img, target_size=(299,299))\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "preds1 = xception_model.predict(np.array([x])).flatten()\n",
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1IJ_WSuB4g3N",
    "outputId": "d241a97b-5b8f-4a64-e2fa-35e4d8d006f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(preds1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4F3pPRakwPK2",
    "outputId": "4c7fd9ce-d0ef-4101-b7c1-00f7044b522e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 969ms/step - loss: 0.6468 - accuracy: 0.8125\n",
      "Validation Accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = vggmodel.evaluate(test_set, steps=test_set.samples // batch_size)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHrICHyrwtg-",
    "outputId": "bfb16111-8459-4503-90eb-72c0b895d298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 0.6900 - accuracy: 0.5938\n",
      "Validation Accuracy: 59.38%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = resnet_model.evaluate(test_set, steps=test_set.samples // batch_size)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SX9CvS-RwudA",
    "outputId": "17fbb1d0-0d4f-4f18-fef9-5cb035d4347d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.4108 - accuracy: 0.8125\n",
      "Validation Accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = inception_model.evaluate(test_set, steps=test_set.samples // batch_size)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQGmobmXwyjn",
    "outputId": "cec6d300-08da-48af-d22f-8301206119a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.4950 - accuracy: 0.7812\n",
      "Validation Accuracy: 78.12%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = densenet_model.evaluate(test_set, steps=test_set.samples // batch_size)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FtjtMVPw1ii",
    "outputId": "152900b4-ebda-45be-8e97-f8fe33ffe129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 957ms/step - loss: 0.4534 - accuracy: 0.7188\n",
      "Validation Accuracy: 71.88%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = xception_model.evaluate(test_set, steps=test_set.samples // batch_size)\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDLeaBCx9_RM",
    "outputId": "06bf38fd-7cdb-4bce-c60a-be79e02fd306"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "vggmodel.save('waves_img.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
